{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c683aa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from config import IMAGE_TRANSFORM\n",
    "import torch\n",
    "import clip\n",
    "import os\n",
    "import math\n",
    "from pytorchvideo import transforms as pv_transforms\n",
    "from torchvision.transforms._transforms_video import NormalizeVideo\n",
    "from pytorchvideo.data.clip_sampling import ConstantClipsPerVideoSampler\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "import av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ab2ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tokens: torch.Size([1, 49, 768])\n",
      "Text tokens: torch.Size([1, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "text = clip.tokenize([\"a diagram\"]).to(device)   # [3, 77]\n",
    "\n",
    "# Get internal layers\n",
    "with torch.no_grad():\n",
    "    # Get image tokens: outputs before final pooling\n",
    "    image_tokens = model.visual.conv1(image)  # [1, C, H/patch, W/patch]\n",
    "    B, C, H, W = image_tokens.shape\n",
    "    image_tokens = image_tokens.reshape(B, C, H*W).permute(0, 2, 1)  # [B, N, C]\n",
    "    print(\"Image tokens:\", image_tokens.shape)\n",
    "\n",
    "    # Get text tokens: token embeddings before pooling\n",
    "    project_to_768 = nn.Linear(512, 768).to(device)\n",
    "    x = project_to_768(model.token_embedding(text))  # [B, N, C]\n",
    "    print(\"Text tokens:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7417410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00569609, 0.00521164, 0.98909223]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"cat.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = CLIP_model.encode_image(image)\n",
    "    text_features = CLIP_model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = CLIP_model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4dc5492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([3, 512]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2248471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_transform_vision_date(image_paths, device=\"cpu\"):\n",
    "    if image_paths is None: return None\n",
    "    image_outputs = []\n",
    "\n",
    "    for image_path in os.listdir(image_paths):\n",
    "        if image_path.endswith(\".jpg\") or image_path.endswith(\".jpeg\") or image_path.endswith(\".png\"):\n",
    "            with open(os.path.join(image_paths, image_path), \"rb\") as im:\n",
    "                image = Image.open(im).convert(\"RGB\")\n",
    "                image = IMAGE_TRANSFORM(img = image)\n",
    "                image_outputs.append(image)\n",
    "    return torch.stack(image_outputs, dim=0)\n",
    "\n",
    "images = load_and_transform_vision_date(\"Data/image_Data\")\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ec3f11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 77])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_and_transform_text(texts, device=\"cpu\"):\n",
    "    if texts is None: return None\n",
    "    tokens = [clip.tokenize(text).unsqueeze(0).to(device) for text in texts]\n",
    "    return torch.cat(tokens, dim=0)\n",
    "tokenized_text = load_and_transform_text([\"cat\", \"dog\"])\n",
    "tokenized_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092393af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def uniform_crop(images, size, spacial_idx, boxes=None, scale_size=None):\n",
    "    assert spacial_idx in [0, 1, 2], \"`spacial_idx` must be 0 -> Left | 1 -> Center | 2 -> Right\"\n",
    "    height = images.shape[2]\n",
    "    width = images.shape[3]\n",
    "    \n",
    "    if scale_size: # Scale size keeps the ratio of original height and widht intact but changes the shorter size to `scale_size`\n",
    "        if height <= width:\n",
    "            width, height = int((width/height)*scale_size), scale_size\n",
    "        else:\n",
    "            width, height = scale_size, int((height/width)*scale_size)\n",
    "        \n",
    "        images = torch.nn.functional.interpolate(\n",
    "            images,\n",
    "            size=(height, width),\n",
    "            mode = \"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "    y_offset = math.ceil((height-size)/2)\n",
    "    x_offset = math.ceil((width-size)/2)\n",
    "\n",
    "    if height <= width:\n",
    "        if spacial_idx == 0:\n",
    "            x_offset = 0\n",
    "        elif spacial_idx == 2:\n",
    "            x_offset = width - size\n",
    "    else:\n",
    "        if spacial_idx == 0:\n",
    "            y_offset = 0\n",
    "        elif spacial_idx == 2:\n",
    "            y_offset = height - size\n",
    "\n",
    "    return images[:, :, y_offset : y_offset + size, x_offset : x_offset + size]\n",
    "\n",
    "images = torch.rand(3, 2, 1080, 1920)\n",
    "cropped = uniform_crop(images, 224, spacial_idx=0)\n",
    "cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352e4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spacial_Crop(nn.Module):\n",
    "    def __init__(self, crop_size: int, num_crops: int = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.crop_size = crop_size\n",
    "        if num_crops == 3:\n",
    "            self.crop_directions = [0, 1, 2] # 0 -> Left | 1 -> Center | 2 -> Right\n",
    "        elif num_crops == 1:\n",
    "            self.crop_directions = [1] # 1 -> Center\n",
    "        else:\n",
    "            raise NotImplementedError(\"Make sure the `num_crops` is either 1 or 3, 3 being default\")\n",
    "    \n",
    "    def forward(self, videos):\n",
    "        res = []\n",
    "        for video in videos:\n",
    "            for spacial_idx in self.crop_directions:\n",
    "                res.append(uniform_crop(video, size=self.crop_size, spacial_idx=spacial_idx))\n",
    "        return torch.stack(res, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd5d921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 3, 2, 224, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_clip_timepoints(clip_sampler, duration):\n",
    "    is_last_clip = False\n",
    "    end = 0.9\n",
    "    clip_time_points = []\n",
    "    while not is_last_clip:\n",
    "        start, end, _, _, is_last_clip = clip_sampler(0.0, duration, annotation=False)\n",
    "        clip_time_points.append((start, end))\n",
    "    return clip_time_points\n",
    "\n",
    "def load_and_transform_video_data(video_paths, device, clip_duration=2, clips_per_video=5):\n",
    "    video_outputs = []\n",
    "    video_transform = transforms.Compose(\n",
    "        [\n",
    "            pv_transforms.ShortSideScale(224),\n",
    "            NormalizeVideo(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                std=(0.26862954, 0.26130258, 0.27577711),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    clip_sampler = ConstantClipsPerVideoSampler(\n",
    "        clip_duration=clip_duration, clips_per_video=clips_per_video, \n",
    "    )\n",
    "\n",
    "    frame_sampler = pv_transforms.UniformTemporalSubsample(num_samples=2)\n",
    "    for video_path in os.listdir(video_paths): \n",
    "        if video_path.endswith(\".mp4\"):\n",
    "            encoded_video = EncodedVideo.from_path(\n",
    "                file_path=f\"Data/Video_Data/{video_path}\",\n",
    "                decode_audio=False,\n",
    "                decoder=\"pyav\"\n",
    "            )\n",
    "\n",
    "            all_frames = []\n",
    "\n",
    "            clip_time_points = get_clip_timepoints(clip_sampler=clip_sampler, duration=encoded_video.duration)\n",
    "\n",
    "            for clip_time_point in clip_time_points:\n",
    "                clip = encoded_video.get_clip(clip_time_point[0], clip_time_point[1])\n",
    "                if clip is None: ValueError(\"No Clip Found\") \n",
    "                frames = frame_sampler(clip[\"video\"]) / 255.0\n",
    "\n",
    "                all_frames.append(frames)\n",
    "            all_videos = [video_transform(frame) for frame in all_frames]\n",
    "            video_outputs.append(Spacial_Crop(crop_size = 224, num_crops = 3)(all_videos))\n",
    "    return torch.stack(video_outputs, dim=0).to(device)\n",
    "    \n",
    "clips = load_and_transform_video_data(\"Data/Video_Data\", device=\"cpu\", clip_duration=2, clips_per_video=5)\n",
    "clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225ec4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993c026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
