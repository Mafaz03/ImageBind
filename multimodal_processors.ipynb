{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba29a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import math\n",
    "from timm.models.layers import trunc_normal_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4ce0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLayer(\n",
      "  (running_mean): tensor((5,), requires_grad=False)\n",
      "  \n",
      "  (linear): Linear(in_features=1, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VerboseNNModule(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_readable_tensor_representation(name: str, tensor: torch.Tensor):\n",
    "        st = (\n",
    "            \"(\" + name + \"): \" + \"tensor(\" + str(tuple(tensor[1].shape)) + \", requires_grad=\" + str(tensor[1].requires_grad) + \")\\n\"\n",
    "            )\n",
    "        return st\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        named_modules = set()\n",
    "        for p in self.named_modules():\n",
    "            named_modules.update(p[0])\n",
    "        named_modules = list(named_modules)\n",
    "\n",
    "        string_repr = \"\"\n",
    "        for p in self.named_parameters():\n",
    "            name = p[0].split(\".\")[0]\n",
    "            if name in named_modules:\n",
    "                string_repr += self.get_readable_tensor_representation(name, p)\n",
    "        \n",
    "        for p in self.named_buffers():\n",
    "            name = p[0].split(\".\")[0]\n",
    "            string_repr += self.get_readable_tensor_representation(name, p)\n",
    "        \n",
    "        return string_repr\n",
    "\n",
    "class MyLayer(VerboseNNModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 2)\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(5))\n",
    "\n",
    "# Instantiate and print the model\n",
    "model = MyLayer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aef52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_causal_attention_mask(context_length):\n",
    "    mask = torch.empty(context_length, context_length, requires_grad=False)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)\n",
    "    return mask\n",
    "\n",
    "build_causal_attention_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94d826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor(VerboseNNModule):\n",
    "    def __init__(self, vocab_size: int, context_length: int, embed_dim: int, causual_mask: bool, \n",
    "                 supply_seq_len_to_head: bool = True, init_param_style: str = \"openclip\"):\n",
    "        \"\"\"\n",
    "        `vocab_size`: Number of tokens in your vocabulary.                 the number of words in your text, so we can map nn.Embedding\n",
    "\t    `context_length`: Maximum number of tokens per input sequence.     usually: 77\n",
    "\t    `embed_dim`: Dimensionality of each token embedding.               usually: 768\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.causual_mask = causual_mask\n",
    "        self.embed_dim = embed_dim\n",
    "        self.supply_seq_len_to_head = supply_seq_len_to_head\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.empty(1, context_length, embed_dim)\n",
    "        )\n",
    "        if causual_mask:\n",
    "            mask = build_causal_attention_mask(context_length)\n",
    "            self.register_buffer(\"mask\", mask) # register the mask as a buffer so it can be moved to the right device\n",
    "        \n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style = \"openclip\"):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        # I did'nt use init_param_style as I was too lazy to implment [CLS]\n",
    "    \n",
    "    def forward(self, text):\n",
    "        token_text = self.token_embedding(text)\n",
    "        token_text = token_text + self.pos_embed\n",
    "        \n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": token_text\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "\n",
    "        if self.supply_seq_len_to_head:\n",
    "            text_lengths = text.argmax(dim = -1)\n",
    "            #  hacky and non-standard way of getting the sequence length.\n",
    "            return_dict[\"head\"] = {\n",
    "                \"seq_len\": text_lengths,\n",
    "            }\n",
    "        if self.causual_mask:\n",
    "            return_dict[\"trunk\"].update({\"attn_mask\": self.mask})\n",
    "        \n",
    "        return return_dict\n",
    "    \n",
    "vocab_size = 100\n",
    "context_length = 77\n",
    "embed_dim = 768\n",
    "\n",
    "# Sample input: batch of 1, padded or truncated to 77 tokens\n",
    "text = torch.randint(0, vocab_size, (2, context_length))  # shape [1, 77]\n",
    "\n",
    "text_processor = TextPreprocessor(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    causual_mask=True,\n",
    "    supply_seq_len_to_head=True\n",
    ")\n",
    "\n",
    "out = text_processor(text)\n",
    "print(out[\"trunk\"][\"tokens\"].shape)  # âžœ [2, 77, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1d3395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 77])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"trunk\"][\"attn_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935c1cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22, 72])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"head\"][\"seq_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5912916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextPreprocessor(\n",
      "  (mask): tensor((77, 77), requires_grad=False)\n",
      "  \n",
      "  (token_embedding): Embedding(100, 768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(text_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9716d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100352, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedGeneric(nn.Module):\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "    \n",
    "    def get_patch_layout(self, image_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros([1,] + image_size)      # 1, C, (T), H, W\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        # print(dummy_out.shape)\n",
    "        embed_dim = dummy_out.shape[1]                    # `embed_dim`    = C        \n",
    "        patch_layout = tuple(dummy_out.shape[2:])         # `patch_layout` = (T), H, W       \n",
    "        num_patches = np.prod(patch_layout)               # `num_patches`  = (T) * H * W       \n",
    "        return embed_dim, patch_layout, num_patches\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.flatten(2)                                  # B, C, (T), H, W -> B, C, (T)*H*W\n",
    "        x = x.transpose(1, 2)                             # B, C, (T)*H*W   -> B, (T)*H*W, C\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "\n",
    "## Testing\n",
    "proj_stem = [\n",
    "    nn.Linear(224, 224),\n",
    "    nn.ReLU()\n",
    "]\n",
    "\n",
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "out = patch_embed(x)\n",
    "out.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0813c2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 3\n",
      "Patch layout: (224, 224)\n",
      "Number of patches: 50176\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 224, 224)\n",
    "embed_dim, patch_layout, num_patches = patch_embed.get_patch_layout(list(x.shape))\n",
    "\n",
    "print(f\"Embed dim: {embed_dim}\")\n",
    "print(f\"Patch layout: {patch_layout}\")\n",
    "print(f\"Number of patches: {num_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb127a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  8.2843e-01,  ...,  1.0000e+00,\n",
      "           1.0243e-04,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  9.2799e-01,  ...,  1.0000e+00,\n",
      "           2.0486e-04,  1.0000e+00],\n",
      "         ...,\n",
      "         [-9.7846e-01, -2.0645e-01, -6.9584e-02,  ...,  9.9980e-01,\n",
      "           1.9767e-02,  9.9980e-01],\n",
      "         [-7.0239e-01,  7.1180e-01,  7.8745e-01,  ...,  9.9979e-01,\n",
      "           1.9870e-02,  9.9980e-01],\n",
      "         [ 2.1945e-01,  9.7562e-01,  9.5167e-01,  ...,  9.9979e-01,\n",
      "           1.9972e-02,  9.9980e-01]]])\n"
     ]
    }
   ],
   "source": [
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    \"\"\"Sinusoid position encoding table\"\"\"\n",
    "\n",
    "    # TODO: make it with torch instead of numpy\n",
    "    def get_position_angle_vec(position):\n",
    "        return [\n",
    "            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n",
    "            for hid_j in range(d_hid)\n",
    "        ]\n",
    "\n",
    "    sinusoid_table = np.array(\n",
    "        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n",
    "    )\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "pos_encoding = get_sinusoid_encoding_table(n_position=196, d_hid=768)\n",
    "print(pos_encoding.shape)  # Output: (1, 4, 8)\n",
    "print(pos_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1390ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interpolate_pos_encoding(npatch_per_image, pos_embed, first_patch_idx: int = 1): \n",
    "    # If CLS present first_patch_idx = 1\n",
    "\n",
    "    assert first_patch_idx == 0 or first_patch_idx == 1, \"CLS can be either present or not present\"\n",
    "    # assert \n",
    "    N = pos_embed.shape[1] - first_patch_idx             # If CLS is present tokens from the 1: to rest are actual stuff\n",
    "    \n",
    "    if npatch_per_image == N:\n",
    "        return pos_embed\n",
    "\n",
    "    class_emb = pos_embed[:, :first_patch_idx]\n",
    "    pos_embed = pos_embed[:, first_patch_idx:]\n",
    "\n",
    "    return torch.cat((class_emb, pos_embed), dim=1)\n",
    "\n",
    "pos_embed = torch.rand(1, 196, 768)\n",
    "interpolate_pos_encoding(npatch_per_image = 196, pos_embed = pos_embed, first_patch_idx=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8659b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_pos_embedding(npatch_per_image, pos_embed, first_patch_idx: int = 1):\n",
    "    return interpolate_pos_encoding(npatch_per_image, pos_embed, first_patch_idx)\n",
    "_get_pos_embedding(npatch_per_image = 196, pos_embed = pos_embed, first_patch_idx=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52301db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporal_posEmbeddingHelper(VerboseNNModule):\n",
    "    def __init__(self, num_patches: int, num_cls_tokens: int, embed_dim: int, learnable: bool):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        self.embed_dim = embed_dim\n",
    "        self.learnable = learnable\n",
    "\n",
    "        self.num_tokens = num_patches + num_cls_tokens\n",
    "\n",
    "        if learnable:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                                torch.zeros(1, self.num_tokens, embed_dim)\n",
    "                            )\n",
    "            trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        else: self.register_buffer(\n",
    "            \"pos_embed\", get_sinusoid_encoding_table(n_position = self.num_tokens, d_hid = embed_dim)\n",
    "            )\n",
    "    \n",
    "    def get_pos_embedding(self, all_vision_tokens):\n",
    "        pos_embed = _get_pos_embedding(\n",
    "            npatch_per_image = all_vision_tokens.size(1) - self.num_cls_tokens,\n",
    "            pos_embed=self.pos_embed,\n",
    "            first_patch_idx=self.num_cls_tokens,\n",
    "        )\n",
    "        return pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a48710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches = 16\n",
    "num_cls_tokens = 1\n",
    "embed_dim = 768\n",
    "\n",
    "# Fake patch embeddings for 2 images\n",
    "batch_size = 2\n",
    "patch_embeddings = torch.randn(batch_size, num_patches, embed_dim)  # [2, 16, 768]\n",
    "\n",
    "# CLS token (typically learned)\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))  # [1, 1, 768]\n",
    "cls_tokens = cls_token.expand(batch_size, -1, -1)       # [2, 1, 768]\n",
    "\n",
    "# Combine CLS and patches\n",
    "all_vision_tokens = torch.cat([cls_tokens, patch_embeddings], dim=1)  # [2, 17, 768]\n",
    "all_vision_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2612608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_helper = SpatioTemporal_posEmbeddingHelper(\n",
    "    num_patches=num_patches,\n",
    "    num_cls_tokens=num_cls_tokens,\n",
    "    embed_dim=embed_dim,\n",
    "    learnable=False  # or True if you want learnable positions\n",
    ")\n",
    "\n",
    "vision_input = torch.randn(batch_size, 3, 224, 224)  # dummy input\n",
    "pos_embed = pos_helper.get_pos_embedding(all_vision_tokens = all_vision_tokens)  # [1, 17, 768] (broadcastable)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07090a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBTProcessor(VerboseNNModule):\n",
    "    def __init__(self, rgbt_stem: PatchEmbedGeneric, img_size: Tuple = [3, 224, 224],\n",
    "                 num_cls_token: int = 1, pos_embed_fn: SpatioTemporal_posEmbeddingHelper = None, \n",
    "                 use_type_embed: bool = False, init_param_style: str = \"openclip\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim, self.patches_layout, self.num_patches = rgbt_stem.get_patch_layout(img_size)\n",
    "        self.num_cls_token = num_cls_token\n",
    "        self.use_type_embed = use_type_embed\n",
    "        self.init_param_style = init_param_style\n",
    "        self.use_pos_embed = pos_embed_fn is not None\n",
    "        self.rgbt_stem = rgbt_stem\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embed_helper = pos_embed_fn(\n",
    "                num_patches = self.num_patches,\n",
    "                num_cls_tokens = self.num_cls_token,\n",
    "                embed_dim = self.embed_dim,\n",
    "                learnable = True\n",
    "            )\n",
    "        \n",
    "        if num_cls_token > 0:\n",
    "            self.cls_tokens = nn.Parameter(\n",
    "                torch.zeros(1, self.num_cls_token, self.embed_dim)\n",
    "            )\n",
    "        if self.use_type_embed: # The model learns to adjust type_embed so that it provides differentiation for different modalities\n",
    "            self.type_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1, self.embed_dim)\n",
    "            )\n",
    "        \n",
    "        self.init_parameters(init_param_style)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, parameter_style):\n",
    "        if parameter_style == \"openclip\":\n",
    "            # OpenCLIP style initialization\n",
    "            scale = self.embed_dim ** -0.5\n",
    "        \n",
    "            if self.use_type_embed:\n",
    "                nn.init.normal_(self.pos_embed_helper.pos_embed)\n",
    "                self.pos_embed_helper.pos_embed *= scale\n",
    "            \n",
    "            if self.num_cls_token > 0:\n",
    "                nn.init.normal_(self.cls_tokens)\n",
    "                self.cls_tokens *= scale\n",
    "        \n",
    "        elif parameter_style == \"vit\":\n",
    "            self.cls_tokens.data.fill_(0)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init {parameter_style}\")\n",
    "        \n",
    "        if self.use_type_embed:\n",
    "            nn.init.normal_(self.type_embed)\n",
    "        \n",
    "    def tokenize_input_and_cls_pos(self, input, stem):\n",
    "        tokens = stem(input)\n",
    "        assert tokens.ndim == 3\n",
    "        assert tokens.shape[-1] == self.embed_dim\n",
    "\n",
    "        B = tokens.shape[0] # batch size\n",
    "        \n",
    "        if self.num_cls_token > 0:\n",
    "            cls_tokens = self.cls_tokens.expand(B, -1, -1)   # Making sure Batches are matching or shape mismatch might occur\n",
    "            tokens = torch.cat([cls_tokens, tokens], dim=1)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            pos_embed = self.pos_embed_helper.get_pos_embedding(all_vision_tokens = tokens)\n",
    "            tokens = tokens + pos_embed\n",
    "        \n",
    "        if self.use_type_embed:\n",
    "            tokens = tokens + self.type_embed.expand(B, -1, -1)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def forward(self, vision = None):\n",
    "        vision_tokens = self.tokenize_input_and_cls_pos(input = vision, stem = self.rgbt_stem)\n",
    "        return_dict = {\n",
    "                        \"trunk\": {\n",
    "                            \"tokens\": vision_tokens\n",
    "                        },\n",
    "                        \"head\": {}\n",
    "                    }\n",
    "        return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3b73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_patches = 16\n",
    "num_cls_tokens = 1\n",
    "embed_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f53cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_stem = [\n",
    "    nn.Linear(224, 224),\n",
    "    nn.ReLU()\n",
    "]\n",
    "\n",
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "\n",
    "pos_helper = SpatioTemporal_posEmbeddingHelper(\n",
    "    num_patches=num_patches,\n",
    "    num_cls_tokens=num_cls_tokens,\n",
    "    embed_dim=embed_dim,\n",
    "    learnable=False  # or True if you want learnable positions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40734a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3900,  0.2066,  0.0901],\n",
       "         [ 1.0476, -1.0794,  1.0924],\n",
       "         [ 1.6253, -0.0469,  1.5444],\n",
       "         ...,\n",
       "         [ 1.3145, -1.5348,  1.7872],\n",
       "         [ 0.8960, -2.5253,  1.2743],\n",
       "         [ 2.8765, -0.9186,  1.1537]],\n",
       "\n",
       "        [[ 2.3900,  0.2066,  0.0901],\n",
       "         [ 0.9397, -1.4308,  1.3125],\n",
       "         [ 1.9686, -0.7751,  1.3938],\n",
       "         ...,\n",
       "         [ 2.7351, -1.5255,  0.1055],\n",
       "         [ 1.4153,  0.4458, -0.9008],\n",
       "         [ 3.8820,  0.4368,  0.0058]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgbt_processor = RGBTProcessor(rgbt_stem = patch_embed,\n",
    "              pos_embed_fn = SpatioTemporal_posEmbeddingHelper, use_type_embed = True\n",
    "              )\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "out = rgbt_processor(x)\n",
    "out[\"trunk\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3c9c3257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50177, 3])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"trunk\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e523136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2a0903dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/xlr6tgyx4t980qpxnnrs12kc0000gn/T/ipykernel_22456/676307527.py:1: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(torch.rand(1,2,3))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0355,  0.7191, -0.7407],\n",
       "         [-1.3586, -0.0918, -1.3073]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal(torch.rand(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d00f905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (224, 224) 50176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RGBTProcessor()"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGBTProcessor(patch_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fcc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100e9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
