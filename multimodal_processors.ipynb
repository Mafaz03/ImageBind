{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba29a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import math\n",
    "from timm.models.layers import trunc_normal_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4ce0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLayer(\n",
      "  (running_mean): tensor((5,), requires_grad=False)\n",
      "  \n",
      "  (linear): Linear(in_features=1, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VerboseNNModule(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_readable_tensor_representation(name: str, tensor: torch.Tensor):\n",
    "        st = (\n",
    "            \"(\" + name + \"): \" + \"tensor(\" + str(tuple(tensor[1].shape)) + \", requires_grad=\" + str(tensor[1].requires_grad) + \")\\n\"\n",
    "            )\n",
    "        return st\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        named_modules = set()\n",
    "        for p in self.named_modules():\n",
    "            named_modules.update(p[0])\n",
    "        named_modules = list(named_modules)\n",
    "\n",
    "        string_repr = \"\"\n",
    "        for p in self.named_parameters():\n",
    "            name = p[0].split(\".\")[0]\n",
    "            if name in named_modules:\n",
    "                string_repr += self.get_readable_tensor_representation(name, p)\n",
    "        \n",
    "        for p in self.named_buffers():\n",
    "            name = p[0].split(\".\")[0]\n",
    "            string_repr += self.get_readable_tensor_representation(name, p)\n",
    "        \n",
    "        return string_repr\n",
    "\n",
    "class MyLayer(VerboseNNModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 2)\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(5))\n",
    "\n",
    "# Instantiate and print the model\n",
    "model = MyLayer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8aef52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_causal_attention_mask(context_length):\n",
    "    mask = torch.empty(context_length, context_length, requires_grad=False)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)\n",
    "    return mask\n",
    "\n",
    "build_causal_attention_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94d826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor(VerboseNNModule):\n",
    "    def __init__(self, vocab_size: int, context_length: int, embed_dim: int, causual_mask: bool, \n",
    "                 supply_seq_len_to_head: bool = True, init_param_style: str = \"openclip\"):\n",
    "        \"\"\"\n",
    "        `vocab_size`: Number of tokens in your vocabulary.                 the number of words in your text, so we can map nn.Embedding\n",
    "\t    `context_length`: Maximum number of tokens per input sequence.     usually: 77\n",
    "\t    `embed_dim`: Dimensionality of each token embedding.               usually: 768\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.causual_mask = causual_mask\n",
    "        self.embed_dim = embed_dim\n",
    "        self.supply_seq_len_to_head = supply_seq_len_to_head\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.empty(1, context_length, embed_dim)\n",
    "        )\n",
    "        if causual_mask:\n",
    "            mask = build_causal_attention_mask(context_length)\n",
    "            self.register_buffer(\"mask\", mask) # register the mask as a buffer so it can be moved to the right device\n",
    "        \n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style = \"openclip\"):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        # I did'nt use init_param_style as I was too lazy to implment [CLS]\n",
    "    \n",
    "    def forward(self, text):\n",
    "        token_text = self.token_embedding(text)\n",
    "        token_text = token_text + self.pos_embed\n",
    "        \n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": token_text\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "\n",
    "        if self.supply_seq_len_to_head:\n",
    "            text_lengths = text.argmax(dim = -1)\n",
    "            #  hacky and non-standard way of getting the sequence length.\n",
    "            return_dict[\"head\"] = {\n",
    "                \"seq_len\": text_lengths,\n",
    "            }\n",
    "        if self.causual_mask:\n",
    "            return_dict[\"trunk\"].update({\"attn_mask\": self.mask})\n",
    "        \n",
    "        return return_dict\n",
    "    \n",
    "vocab_size = 100\n",
    "context_length = 77\n",
    "embed_dim = 768\n",
    "\n",
    "# Sample input: batch of 1, padded or truncated to 77 tokens\n",
    "text = torch.randint(0, vocab_size, (2, context_length))  # shape [1, 77]\n",
    "\n",
    "text_processor = TextPreprocessor(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    causual_mask=True,\n",
    "    supply_seq_len_to_head=True\n",
    ")\n",
    "\n",
    "out = text_processor(text)\n",
    "print(out[\"trunk\"][\"tokens\"].shape)  # âžœ [2, 77, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1d3395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 77])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"trunk\"][\"attn_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935c1cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 44])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"head\"][\"seq_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5912916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextPreprocessor(\n",
      "  (mask): tensor((77, 77), requires_grad=False)\n",
      "  \n",
      "  (token_embedding): Embedding(100, 768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(text_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9716d23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32768, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedGeneric(nn.Module):\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj_stem = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "    \n",
    "    def get_patch_layout(self, image_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros([1,]) + image_size    # 1, C, (T), H, W\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        \n",
    "        embed_dim = dummy_out[1]                          # `embed_dim`    = C        \n",
    "        patch_layout = tuple(dummy_out.shape[2:])         # `patch_layout` = (T), H, W       \n",
    "        num_patches = np.prod(patch_layout)               # `num_patches`  = (T) * H * W       \n",
    "        return embed_dim, patch_layout, num_patches\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x.flatten(2)                                  # B, C, (T), H, W -> B, C, (T)*H*W\n",
    "        x = x.transpose(1, 2)                             # B, C, (T)*H*W   -> B, (T)*H*W, C\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "\n",
    "## Testing\n",
    "proj_stem = [\n",
    "    nn.Conv3d(3, 16, kernel_size=3, stride=2, padding=1),  # Assume input is (B, 3, 8, 64, 64)\n",
    "    nn.ReLU()\n",
    "]\n",
    "\n",
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "x = torch.randn(2, 3, 8, 64, 64)\n",
    "\n",
    "out = patch_embed(x)\n",
    "out.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb127a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9996e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    \"\"\"Sinusoid position encoding table\"\"\"\n",
    "\n",
    "    # TODO: make it with torch instead of numpy\n",
    "    def get_position_angle_vec(position):\n",
    "        return [\n",
    "            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n",
    "            for hid_j in range(d_hid)\n",
    "        ]\n",
    "\n",
    "    sinusoid_table = np.array(\n",
    "        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n",
    "    )\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "pos_encoding = get_sinusoid_encoding_table(n_position=4, d_hid=8)\n",
    "print(pos_encoding.shape)  # Output: (1, 4, 8)\n",
    "print(pos_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1390ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interpolate_pos_encoding(npatch_per_image, pos_embed, first_patch_idx: int = 1): \n",
    "    # If CLS present first_patch_idx = 1\n",
    "\n",
    "    assert first_patch_idx == 0 or first_patch_idx == 1, \"CLS can be either present or not present\"\n",
    "    # assert \n",
    "    N = pos_embed.shape[1] - first_patch_idx             # If CLS is present tokens from the 1: to rest are actual stuff\n",
    "    \n",
    "    if npatch_per_image == N:\n",
    "        return pos_embed\n",
    "\n",
    "    class_emb = pos_embed[:, :first_patch_idx]\n",
    "    pos_embed = pos_embed[:, first_patch_idx:]\n",
    "\n",
    "    return torch.cat((class_emb, pos_embed), dim=1)\n",
    "\n",
    "pos_embed = torch.rand(1, 196, 768)\n",
    "\n",
    "interpolate_pos_encoding(npatch_per_image = 196, pos_embed = pos_embed, first_patch_idx=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8659b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_pos_embedding(npatch_per_image, pos_embed, first_patch_idx: int = 1):\n",
    "    return interpolate_pos_encoding(npatch_per_image, pos_embed, first_patch_idx)\n",
    "_get_pos_embedding(npatch_per_image = 196, pos_embed = pos_embed, first_patch_idx=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52301db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporal_posEmbeddingHelper(VerboseNNModule):\n",
    "    def __init__(self, num_patches: int, num_cls_tokens: int, embed_dim: int, learnable: bool):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        self.embed_dim = embed_dim\n",
    "        self.learnable = learnable\n",
    "\n",
    "        self.num_tokens = num_patches + num_cls_tokens\n",
    "\n",
    "        if learnable:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                                torch.zeros(1, self.num_tokens, embed_dim)\n",
    "                            )\n",
    "            trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        else: self.register_buffer(\n",
    "            \"pos_embed\", get_sinusoid_encoding_table(n_position = self.num_tokens, d_hid = embed_dim)\n",
    "            )\n",
    "    \n",
    "    def get_pos_embedding(self, all_vision_tokens):\n",
    "        pos_embed = _get_pos_embedding(\n",
    "            npatch_per_image = all_vision_tokens.size(1) - self.num_cls_tokens,\n",
    "            pos_embed=self.pos_embed,\n",
    "            first_patch_idx=self.num_cls_tokens,\n",
    "        )\n",
    "        return pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5a48710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches = 16\n",
    "num_cls_tokens = 1\n",
    "embed_dim = 768\n",
    "\n",
    "# Fake patch embeddings for 2 images\n",
    "batch_size = 2\n",
    "patch_embeddings = torch.randn(batch_size, num_patches, embed_dim)  # [2, 16, 768]\n",
    "\n",
    "# CLS token (typically learned)\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))  # [1, 1, 768]\n",
    "cls_tokens = cls_token.expand(batch_size, -1, -1)       # [2, 1, 768]\n",
    "\n",
    "# Combine CLS and patches\n",
    "all_vision_tokens = torch.cat([cls_tokens, patch_embeddings], dim=1)  # [2, 17, 768]\n",
    "all_vision_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2612608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_helper = SpatioTemporal_posEmbeddingHelper(\n",
    "    num_patches=num_patches,\n",
    "    num_cls_tokens=num_cls_tokens,\n",
    "    embed_dim=embed_dim,\n",
    "    learnable=False  # or True if you want learnable positions\n",
    ")\n",
    "\n",
    "vision_input = torch.randn(batch_size, 3, 224, 224)  # dummy input\n",
    "pos_embed = pos_helper.get_pos_embedding(all_vision_tokens=all_vision_tokens)  # [1, 17, 768] (broadcastable)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07090a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e7051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
