{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e26713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch import nn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f190d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import LearnableLogitScaling, SelectElement, Normalize, SelectEOSandProject\n",
    "from multimodal_processors import TextPreprocessor, PadIm2Video, PatchEmbedGeneric, SpatioTemporal_posEmbeddingHelper, RGBTProcessor\n",
    "from transformer import MultiheadAttention, SimpleTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a1d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModalityType = SimpleNamespace(\n",
    "    VISION=\"vision\",\n",
    "    TEXT=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3eb5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModalityType = SimpleNamespace(\n",
    "    VISION=\"vision\",\n",
    "    TEXT=\"text\",\n",
    ")\n",
    "\n",
    "\n",
    "class ImageBindModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        kernel_size=(2, 14, 14),\n",
    "    \n",
    "        out_embed_dim=768,\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "        \n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_preprocessors = self._create_modality_preprocessors(\n",
    "            video_frames,\n",
    "            vision_embed_dim,\n",
    "            kernel_size,\n",
    "            text_embed_dim,\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.modality_trunks = self._create_modality_trunks(\n",
    "            vision_embed_dim,\n",
    "            vision_num_blocks,\n",
    "            vision_num_heads,\n",
    "            text_embed_dim,\n",
    "            text_num_blocks,\n",
    "            text_num_heads,\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.modality_heads = self._create_modality_heads(\n",
    "            out_embed_dim,\n",
    "            vision_embed_dim,\n",
    "            text_embed_dim,\n",
    "            \n",
    "        )\n",
    "\n",
    "        self.modality_postprocessors = self._create_modality_postprocessors(\n",
    "            out_embed_dim\n",
    "        )\n",
    "\n",
    "    def _create_modality_preprocessors(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        vision_embed_dim=1024,\n",
    "        kernel_size=(2, 14, 14),\n",
    "        text_embed_dim=768,\n",
    "    ):\n",
    "        rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem=[\n",
    "                PadIm2Video(pad_type=\"repeat\", ntimes=2),\n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        rgbt_preprocessor = RGBTProcessor(\n",
    "            img_size=[3, video_frames, 224, 224],\n",
    "            num_cls_token=1,\n",
    "            pos_embed_fn=partial(SpatioTemporal_posEmbeddingHelper, learnable=True),\n",
    "            rgbt_stem=rgbt_stem,\n",
    "        )\n",
    "\n",
    "        text_preprocessor = TextPreprocessor(\n",
    "            context_length=77,\n",
    "            vocab_size=49408,\n",
    "            embed_dim=text_embed_dim,\n",
    "            causual_mask=True,\n",
    "        )\n",
    "\n",
    "\n",
    "        modality_preprocessors = {\n",
    "            ModalityType.VISION: rgbt_preprocessor,\n",
    "            ModalityType.TEXT: text_preprocessor,\n",
    "        }\n",
    "\n",
    "        return nn.ModuleDict(modality_preprocessors)\n",
    "\n",
    "    def _create_modality_trunks(\n",
    "        self,\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "    ):\n",
    "        def instantiate_trunk(\n",
    "            embed_dim, num_blocks, num_heads, pre_transformer_ln, add_bias_kv, drop_path\n",
    "        ):\n",
    "            return SimpleTransformer(\n",
    "                embed_dim=embed_dim,\n",
    "                num_blocks=num_blocks,\n",
    "                ffn_dropout_rate=0.0,\n",
    "                drop_path_rate=drop_path,\n",
    "                attn_target=\n",
    "                    MultiheadAttention(embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    bias=True,\n",
    "                    add_bias_kv=add_bias_kv,),\n",
    "                \n",
    "                pre_transformer_layer=nn.Sequential(\n",
    "                    nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "                    if pre_transformer_ln\n",
    "                    else nn.Identity(),\n",
    "                ),\n",
    "                post_transformer_layer=nn.Identity(),\n",
    "            )\n",
    "\n",
    "        modality_trunks = {}\n",
    "        modality_trunks[ModalityType.VISION] = instantiate_trunk(\n",
    "            vision_embed_dim,\n",
    "            vision_num_blocks,\n",
    "            vision_num_heads,\n",
    "            pre_transformer_ln=True,\n",
    "            add_bias_kv=False,\n",
    "            drop_path=0.0,\n",
    "        )\n",
    "        modality_trunks[ModalityType.TEXT] = instantiate_trunk(\n",
    "            text_embed_dim,\n",
    "            text_num_blocks,\n",
    "            text_num_heads,\n",
    "            pre_transformer_ln=False,\n",
    "            add_bias_kv=False,\n",
    "            drop_path=0.0,\n",
    "        )\n",
    "    \n",
    "\n",
    "        return nn.ModuleDict(modality_trunks)\n",
    "\n",
    "    def _create_modality_heads(\n",
    "        self,\n",
    "        out_embed_dim,\n",
    "        vision_embed_dim,\n",
    "        text_embed_dim,\n",
    "    ):\n",
    "        modality_heads = {}\n",
    "\n",
    "        modality_heads[ModalityType.VISION] = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=vision_embed_dim, eps=1e-6),\n",
    "            SelectElement(index=0),\n",
    "            nn.Linear(vision_embed_dim, out_embed_dim, bias=False),\n",
    "        )\n",
    "\n",
    "        modality_heads[ModalityType.TEXT] = SelectEOSandProject(\n",
    "            proj=nn.Sequential(\n",
    "                nn.LayerNorm(normalized_shape=text_embed_dim, eps=1e-6),\n",
    "                nn.Linear(text_embed_dim, out_embed_dim, bias=False),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return nn.ModuleDict(modality_heads)\n",
    "\n",
    "    def _create_modality_postprocessors(self, out_embed_dim):\n",
    "        modality_postprocessors = {}\n",
    "\n",
    "        modality_postprocessors[ModalityType.VISION] = Normalize(dim=-1)\n",
    "        modality_postprocessors[ModalityType.TEXT] = nn.Sequential(\n",
    "            Normalize(dim=-1), LearnableLogitScaling(learnable=True)\n",
    "        )\n",
    "\n",
    "        return nn.ModuleDict(modality_postprocessors)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = {}\n",
    "        for modality_key, modality_value in inputs.items():\n",
    "            reduce_list = (\n",
    "                modality_value.ndim >= 5\n",
    "            )  # Audio and Video inputs consist of multiple clips\n",
    "            if reduce_list:\n",
    "                B, S = modality_value.shape[:2]\n",
    "                modality_value = modality_value.reshape(\n",
    "                    B * S, *modality_value.shape[2:]\n",
    "                )\n",
    "\n",
    "            if modality_value is not None:\n",
    "                modality_value = self.modality_preprocessors[modality_key](\n",
    "                    **{modality_key: modality_value}\n",
    "                )\n",
    "                trunk_inputs = modality_value[\"trunk\"]\n",
    "                head_inputs = modality_value[\"head\"]\n",
    "                modality_value = self.modality_trunks[modality_key](**trunk_inputs)\n",
    "                modality_value = self.modality_heads[modality_key](\n",
    "                    modality_value, **head_inputs\n",
    "                )\n",
    "                modality_value = self.modality_postprocessors[modality_key](\n",
    "                    modality_value\n",
    "                )\n",
    "\n",
    "                if reduce_list:\n",
    "                    modality_value = modality_value.reshape(B, S, -1)\n",
    "                    modality_value = modality_value.mean(dim=1)\n",
    "\n",
    "                outputs[modality_key] = modality_value\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7c1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "context_length = 77\n",
    "# embed_dim = 8\n",
    "\n",
    "\n",
    "text = torch.randint(0, vocab_size, (2, context_length)) # text: [2, 77]\n",
    "image = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "inputs = {\n",
    "    ModalityType.VISION: image,\n",
    "    ModalityType.TEXT: text\n",
    "}\n",
    "with torch.no_grad():\n",
    "    imagebindmodel = ImageBindModel()\n",
    "    outputs = imagebindmodel(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1c804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1f837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBindModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        kernel_size=(2, 14, 14),\n",
    "        out_embed_dim=768,\n",
    "\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_preprocessors = self._create_modality_preprocessors(\n",
    "            video_frames,\n",
    "            vision_embed_dim,\n",
    "            kernel_size,\n",
    "            text_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.modality_trunks = self._create_modality_trunks(\n",
    "            vision_embed_dim,\n",
    "            vision_num_blocks,\n",
    "            vision_num_heads,\n",
    "            text_embed_dim,\n",
    "            text_num_blocks,\n",
    "            text_num_heads,\n",
    "        )\n",
    "\n",
    "        self.modality_heads = self._create_modality_heads(\n",
    "            out_embed_dim,\n",
    "            vision_embed_dim,\n",
    "            text_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.modality_postprocessors = self._create_modality_postprocessors()\n",
    "\n",
    "    def _create_modality_preprocessors(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        vision_embed_dim=1024,\n",
    "        kernel_size = (2, 14, 14),\n",
    "\n",
    "        text_embed_dim=768,\n",
    "    ):\n",
    "        proj_stem = [\n",
    "                PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim, #\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            \n",
    "            ]\n",
    "        rgbt_stem = PatchEmbedGeneric(proj_stem = proj_stem)\n",
    "\n",
    "        rgbt_preprocessor = RGBTProcessor(\n",
    "            rgbt_stem = rgbt_stem,\n",
    "            img_size = [3, video_frames, 224, 224],\n",
    "            num_cls_token=1,\n",
    "            pos_embed_fn = SpatioTemporal_posEmbeddingHelper,\n",
    "        )\n",
    "\n",
    "        text_preprocessor = TextPreprocessor(\n",
    "            context_length = 77,\n",
    "            vocab_size = 49408,\n",
    "            embed_dim=text_embed_dim,\n",
    "            causual_mask=True,\n",
    "        )\n",
    "\n",
    "        modality_preprocessors = {\n",
    "            ModalityType.VISION: rgbt_preprocessor,\n",
    "            ModalityType.TEXT: text_preprocessor,\n",
    "        }\n",
    "\n",
    "        return nn.ModuleDict(modality_preprocessors)\n",
    "\n",
    "    def _create_modality_trunks(\n",
    "        self,\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "\n",
    "    ):\n",
    "        def instantiate_trunk(embed_dim, num_blocks, drop_path, num_heads, add_bias_kv, pre_transformer_ln):\n",
    "            multihead_cls = partial(\n",
    "                MultiheadAttention,\n",
    "                embed_dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                bias=True,\n",
    "                add_bias_kv=add_bias_kv,\n",
    "            )\n",
    "            \n",
    "            simple_transformer = SimpleTransformer(\n",
    "                embed_dim = embed_dim,\n",
    "                num_blocks = num_blocks,\n",
    "                ffn_dropout_rate=0.0,\n",
    "                drop_path_rate = drop_path,\n",
    "                attn_target = multihead_cls(),\n",
    "                    \n",
    "                # I already maked sure that the shape is aligned when using MultiheadAttention so no need of rearrangement now\n",
    "                pre_transformer_layer = nn.LayerNorm(embed_dim) if pre_transformer_ln else nn.Identity(),\n",
    "                post_transformer_layer = nn.Identity()\n",
    "            )\n",
    "            return simple_transformer\n",
    "        \n",
    "        modality_trunks = {}\n",
    "        modality_trunks[ModalityType.VISION] = instantiate_trunk(embed_dim = vision_embed_dim, num_blocks = vision_num_blocks, num_heads = vision_num_heads, drop_path = 0.0, pre_transformer_ln=True, add_bias_kv=False,)\n",
    "        modality_trunks[ModalityType.TEXT] = instantiate_trunk(embed_dim = text_embed_dim, num_blocks = text_num_blocks, num_heads = text_num_heads, drop_path = 0.0, pre_transformer_ln=False, add_bias_kv=False,)\n",
    "\n",
    "        return nn.ModuleDict(modality_trunks)\n",
    "    \n",
    "    def _create_modality_heads(\n",
    "        self,\n",
    "        out_embed_dim,\n",
    "        vision_embed_dim,\n",
    "        text_embed_dim,\n",
    "    ):\n",
    "        modality_heads = {}\n",
    "\n",
    "        modality_heads[ModalityType.VISION] = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape = vision_embed_dim, eps=1e-6),\n",
    "            SelectElement(index = 0),\n",
    "            nn.Linear(vision_embed_dim, out_embed_dim, bias = False)\n",
    "        )\n",
    "\n",
    "        modality_heads[ModalityType.TEXT] = SelectEOSandProject(\n",
    "            proj = nn.Sequential(\n",
    "                nn.LayerNorm(normalized_shape=text_embed_dim, eps=1e-6),\n",
    "                nn.Linear(text_embed_dim, out_embed_dim, bias=False),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return nn.ModuleDict(modality_heads)\n",
    "\n",
    "    def _create_modality_postprocessors(self):\n",
    "        modality_postprocessors = {}\n",
    "\n",
    "        modality_postprocessors[ModalityType.VISION] = Normalize(dim=-1)\n",
    "        modality_postprocessors[ModalityType.TEXT] = nn.Sequential(\n",
    "            Normalize(dim=-1), LearnableLogitScaling(learnable=True)\n",
    "        )\n",
    "\n",
    "        return nn.ModuleDict(modality_postprocessors)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = {}\n",
    "        for modality_key, modality_value in inputs.items():\n",
    "            reduce_list = (modality_value.ndim >= 5)     # Because video's ndim is 5 (B, C, T, H, W)\n",
    "            if reduce_list:\n",
    "                B, S = modality_value.shape[:2]\n",
    "                modality_value = modality_value.reshape(B*S, *modality_value.shape[2:])\n",
    "            \n",
    "            if modality_value is not None:\n",
    "                modality_value = self.modality_preprocessors[modality_key](**{modality_key: modality_value}) # Access the forward function of the classes\n",
    "\n",
    "                trunk_inputs = modality_value[\"trunk\"]\n",
    "                head_inputs = modality_value[\"head\"]\n",
    "                \n",
    "                modality_value = self.modality_trunks[modality_key](**trunk_inputs)                # Access the forward function of the classes\n",
    "                modality_value = self.modality_heads[modality_key](modality_value, **head_inputs) # Access the forward function of the classes\n",
    "                modality_value = self.modality_postprocessors[modality_key](modality_value)\n",
    "\n",
    "            if reduce_list:\n",
    "                modality_value = modality_value.reshape(B, S, -1)\n",
    "                modality_value = modality_value.mean(dim=1)\n",
    "\n",
    "            outputs[modality_key] = modality_value\n",
    "            \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f552108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vision': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'text': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "context_length = 77\n",
    "\n",
    "\n",
    "text = torch.randint(0, vocab_size, (2, context_length)) # text: [2, 77]\n",
    "image = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "inputs = {\n",
    "    ModalityType.VISION: image,\n",
    "    ModalityType.TEXT: text\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    # imagebindmodel = ImageBindModel()\n",
    "    outputs = imagebindmodel(inputs)\n",
    "outputs  # don't worry about the zeros we initialized it like that, this will update during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb5a932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): RGBTProcessor(\n",
       "    (rgbt_stem): PatchEmbedGeneric(\n",
       "      (proj): Sequential(\n",
       "        (0): PadIm2Video()\n",
       "        (1): Conv3d(3, 1024, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (pos_embed_helper): SpatioTemporal_posEmbeddingHelper()\n",
       "  )\n",
       "  (text): TextPreprocessor(\n",
       "    (mask): tensor((77, 77), requires_grad=False)\n",
       "    \n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel = ImageBindModel()\n",
    "imagebindmodel.modality_preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7927a618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57, 10, 71, 24, 78, 45, 96, 92, 33, 51, 71, 88, 27, 99, 99, 36, 99, 99,\n",
       "         27, 34, 62, 89, 83, 11, 67, 37, 95, 27, 76, 87, 97,  5, 26, 29, 16, 69,\n",
       "         75, 84, 28, 47, 94, 82, 50, 41, 62,  2,  4,  8, 71, 61, 61,  1, 86, 79,\n",
       "          2, 23, 64, 57, 60, 19, 66, 84, 15, 17, 25, 95, 56, 56, 45,  9, 21, 22,\n",
       "         67, 74, 70, 58, 45],\n",
       "        [ 0, 84, 70, 84, 10, 55,  5, 16, 49, 17, 25, 83, 21, 62, 26, 48, 17, 28,\n",
       "         82, 39, 69,  9, 21, 62, 14,  7,  8, 31, 94,  5, 52,  8, 70, 98, 30, 22,\n",
       "         26, 67, 29, 26, 77, 91, 94, 55, 11, 30, 71, 35, 38, 99,  6, 42, 72, 83,\n",
       "         32, 83, 45, 62, 47, 65, 55, 94, 35,  9, 90, 33, 31, 64, 98, 22, 16, 75,\n",
       "         14, 88, 72, 33, 83]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "context_length = 77\n",
    "embed_dim = 768\n",
    "\n",
    "# Sample input: batch of 1, padded or truncated to 77 tokens\n",
    "text = torch.randint(0, vocab_size, (2, context_length))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcf9012e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trunk': {'tokens': tensor([[[-0.0278, -0.0147,  0.0142,  ...,  0.0379,  0.0231, -0.0007],\n",
       "           [ 0.0036,  0.0190, -0.0072,  ...,  0.0214, -0.0359, -0.0055],\n",
       "           [-0.0086,  0.0439,  0.0641,  ...,  0.0344,  0.0206, -0.0269],\n",
       "           ...,\n",
       "           [-0.0097, -0.0564,  0.0203,  ..., -0.0143,  0.0201, -0.0112],\n",
       "           [-0.0503, -0.0397, -0.0054,  ..., -0.0036,  0.0030, -0.0224],\n",
       "           [-0.0284,  0.0165,  0.0259,  ..., -0.0183, -0.0066,  0.0612]],\n",
       "  \n",
       "          [[-0.0011, -0.0180,  0.0150,  ...,  0.0263,  0.0206,  0.0030],\n",
       "           [-0.0132,  0.0372, -0.0092,  ..., -0.0099, -0.0692, -0.0601],\n",
       "           [-0.0291, -0.0221,  0.0676,  ...,  0.0514,  0.0277, -0.0457],\n",
       "           ...,\n",
       "           [-0.0138, -0.0175,  0.0171,  ..., -0.0866, -0.0165,  0.0325],\n",
       "           [ 0.0122, -0.0165, -0.0106,  ..., -0.0047, -0.0123, -0.0070],\n",
       "           [ 0.0167,  0.0090,  0.0091,  ..., -0.0222, -0.0303,  0.0238]]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  'attn_mask': tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "          [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "          [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]])},\n",
       " 'head': {'seq_len': tensor([13, 49])}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_preprocessors['text'](**{'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db46fad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): SimpleTransformer(\n",
       "    (pre_transformer_layer): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (post_transformer_layer): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (1): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (2): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (3): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (4): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (5): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (6): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (7): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (8): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (9): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (10): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (11): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (12): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (13): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (14): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (15): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (16): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (17): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (18): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (19): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (20): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (21): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (22): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (23): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text): SimpleTransformer(\n",
       "    (pre_transformer_layer): Identity()\n",
       "    (post_transformer_layer): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (1): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (2): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (3): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (4): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (5): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (6): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (7): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (8): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (9): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (10): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (11): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_trunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3daa7cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): SelectElement()\n",
       "    (2): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  )\n",
       "  (text): SelectEOSandProject(\n",
       "    (proj): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3468611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): Normalize()\n",
       "  (text): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_postprocessors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
