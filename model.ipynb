{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e26713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f190d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import LearnableLogitScaling, SelectElement, Normalize, SelectEOSandProject\n",
    "from multimodal_processors import TextPreprocessor, Im2Video, PadIm2Video, PatchEmbedGeneric, SpatioTemporal_posEmbeddingHelper, RGBTProcessor\n",
    "from transformer import MultiheadAttention, SimpleTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a1d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModalityType = SimpleNamespace(\n",
    "    VISION=\"vision\",\n",
    "    TEXT=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae1f837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBindModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        kernel_size=(2, 14, 14),\n",
    "        out_embed_dim=768,\n",
    "\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_preprocessors = self._create_modality_preprocessors(\n",
    "            video_frames,\n",
    "            vision_embed_dim,\n",
    "            kernel_size,\n",
    "            text_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.modality_trunks = self._create_modality_trunks(\n",
    "            vision_embed_dim,\n",
    "            vision_num_blocks,\n",
    "            vision_num_heads,\n",
    "            text_embed_dim,\n",
    "            text_num_blocks,\n",
    "            text_num_heads,\n",
    "        )\n",
    "\n",
    "        self.modality_heads = self._create_modality_heads(\n",
    "            out_embed_dim,\n",
    "            vision_embed_dim,\n",
    "            text_embed_dim,\n",
    "        )\n",
    "\n",
    "    def _create_modality_preprocessors(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        vision_embed_dim=1024,\n",
    "        kernel_size=(2, 14, 14),\n",
    "\n",
    "        text_embed_dim=768,\n",
    "    ):\n",
    "        rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem =\n",
    "            [\n",
    "                PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        rgbt_preprocessor = RGBTProcessor(\n",
    "            rgbt_stem = rgbt_stem,\n",
    "            img_size = [3, video_frames, 224, 224],\n",
    "            num_cls_token=1,\n",
    "            pos_embed_fn=partial(SpatioTemporal_posEmbeddingHelper, learnable=True),\n",
    "        )\n",
    "\n",
    "        text_preprocessor = TextPreprocessor(\n",
    "            context_length = 77,\n",
    "            vocab_size = 49408,\n",
    "            embed_dim=text_embed_dim,\n",
    "            causual_mask=True,\n",
    "        )\n",
    "\n",
    "        modality_preprocessors = {\n",
    "            ModalityType.VISION: rgbt_preprocessor,\n",
    "            ModalityType.TEXT: text_preprocessor,\n",
    "        }\n",
    "\n",
    "        return nn.ModuleDict(modality_preprocessors)\n",
    "\n",
    "    def _create_modality_trunks(\n",
    "        self,\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "\n",
    "    ):\n",
    "        def instantiate_trunk(embed_dim, num_blocks, drop_path, num_heads, add_bias_kv, pre_transformer_ln):\n",
    "            simple_transformer = SimpleTransformer(\n",
    "                embed_dim = embed_dim,\n",
    "                num_blocks = num_blocks,\n",
    "                ffn_dropout_rate=0.0,\n",
    "                drop_path_rate = drop_path,\n",
    "                attn_target = \n",
    "                    MultiheadAttention(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    bias=True,\n",
    "                    add_bias_kv=add_bias_kv,\n",
    "                ),\n",
    "                # I already maked sure that the shape is aligned when using MultiheadAttention so no need of rearrangement now\n",
    "                pre_transformer_layer = nn.LayerNorm(embed_dim) if pre_transformer_ln else nn.Identity(),\n",
    "                post_transformer_layer = nn.Identity()\n",
    "            )\n",
    "            return simple_transformer\n",
    "        \n",
    "        modality_trunks = {}\n",
    "        modality_trunks[ModalityType.VISION] = instantiate_trunk(embed_dim = vision_embed_dim, num_blocks = vision_num_blocks, num_heads = vision_num_heads, drop_path = 0.0, pre_transformer_ln=True, add_bias_kv=False,)\n",
    "        modality_trunks[ModalityType.TEXT] = instantiate_trunk(embed_dim = text_embed_dim, num_blocks = text_num_blocks, num_heads = text_num_heads, drop_path = 0.0, pre_transformer_ln=False, add_bias_kv=False,)\n",
    "        \n",
    "        return nn.ModuleDict(modality_trunks)\n",
    "    \n",
    "    def _create_modality_heads(\n",
    "        self,\n",
    "        out_embed_dim,\n",
    "        vision_embed_dim,\n",
    "        text_embed_dim,\n",
    "    ):\n",
    "        modality_heads = {}\n",
    "\n",
    "        modality_heads[ModalityType.VISION] = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape = vision_embed_dim, eps=1e-6),\n",
    "            SelectElement(index = 0),\n",
    "            nn.Linear(vision_embed_dim, out_embed_dim, bias = False)\n",
    "        )\n",
    "\n",
    "        modality_heads[ModalityType.TEXT] = SelectEOSandProject(\n",
    "            proj = nn.Sequential(\n",
    "                nn.LayerNorm(normalized_shape=text_embed_dim, eps=1e-6),\n",
    "                nn.Linear(text_embed_dim, out_embed_dim, bias=False),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return nn.ModuleDict(modality_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb5a932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): RGBTProcessor(\n",
       "    (rgbt_stem): PatchEmbedGeneric(\n",
       "      (proj): Sequential(\n",
       "        (0): PadIm2Video()\n",
       "        (1): Conv3d(3, 1024, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (pos_embed_helper): SpatioTemporal_posEmbeddingHelper()\n",
       "  )\n",
       "  (text): TextPreprocessor(\n",
       "    (mask): tensor((77, 77), requires_grad=False)\n",
       "    \n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel = ImageBindModel()\n",
    "imagebindmodel.modality_preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db46fad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): SimpleTransformer(\n",
       "    (pre_transformer_layer): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (post_transformer_layer): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (1): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (2): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (3): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (4): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (5): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (6): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (7): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (8): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (9): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (10): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (11): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (12): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (13): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (14): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (15): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (16): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (17): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (18): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (19): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (20): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (21): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (22): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (23): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text): SimpleTransformer(\n",
       "    (pre_transformer_layer): Identity()\n",
       "    (post_transformer_layer): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (1): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (2): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (3): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (4): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (5): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (6): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (7): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (8): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (9): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (10): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (11): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_trunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3daa7cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): SelectElement()\n",
       "    (2): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  )\n",
       "  (text): SelectEOSandProject(\n",
       "    (proj): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468611c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
