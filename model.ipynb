{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e26713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch import nn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f190d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import LearnableLogitScaling, SelectElement, Normalize, SelectEOSandProject\n",
    "from multimodal_processors import TextPreprocessor, PadIm2Video, PatchEmbedGeneric, SpatioTemporal_posEmbeddingHelper, RGBTProcessor\n",
    "from transformer import MultiheadAttention, SimpleTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a1d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModalityType = SimpleNamespace(\n",
    "    VISION=\"vision\",\n",
    "    TEXT=\"text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1f837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageBindModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        kernel_size=(2, 14, 14),\n",
    "        out_embed_dim=768,\n",
    "\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_preprocessors = self._create_modality_preprocessors(\n",
    "            video_frames,\n",
    "            vision_embed_dim,\n",
    "            kernel_size,\n",
    "            text_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.modality_trunks = self._create_modality_trunks(\n",
    "            vision_embed_dim,\n",
    "            vision_num_blocks,\n",
    "            vision_num_heads,\n",
    "            text_embed_dim,\n",
    "            text_num_blocks,\n",
    "            text_num_heads,\n",
    "        )\n",
    "\n",
    "        self.modality_heads = self._create_modality_heads(\n",
    "            out_embed_dim,\n",
    "            vision_embed_dim,\n",
    "            text_embed_dim,\n",
    "        )\n",
    "\n",
    "        self.modality_postprocessors = self._create_modality_postprocessors()\n",
    "\n",
    "    def _create_modality_preprocessors(\n",
    "        self,\n",
    "        video_frames=2,\n",
    "        vision_embed_dim=1024,\n",
    "        kernel_size=(2, 14, 14),\n",
    "\n",
    "        text_embed_dim=768,\n",
    "    ):\n",
    "        rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem =\n",
    "            [\n",
    "                PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        rgbt_preprocessor = RGBTProcessor(\n",
    "            rgbt_stem = rgbt_stem,\n",
    "            img_size = [3, video_frames, 224, 224],\n",
    "            num_cls_token=1,\n",
    "            pos_embed_fn=partial(SpatioTemporal_posEmbeddingHelper, learnable=True),\n",
    "        )\n",
    "\n",
    "        text_preprocessor = TextPreprocessor(\n",
    "            context_length = 77,\n",
    "            vocab_size = 49408,\n",
    "            embed_dim=text_embed_dim,\n",
    "            causual_mask=True,\n",
    "        )\n",
    "\n",
    "        modality_preprocessors = {\n",
    "            ModalityType.VISION: rgbt_preprocessor,\n",
    "            ModalityType.TEXT: text_preprocessor,\n",
    "        }\n",
    "\n",
    "        return nn.ModuleDict(modality_preprocessors)\n",
    "\n",
    "    def _create_modality_trunks(\n",
    "        self,\n",
    "        vision_embed_dim=1024,\n",
    "        vision_num_blocks=24,\n",
    "        vision_num_heads=16,\n",
    "\n",
    "        text_embed_dim=768,\n",
    "        text_num_blocks=12,\n",
    "        text_num_heads=12,\n",
    "\n",
    "    ):\n",
    "        def instantiate_trunk(embed_dim, num_blocks, drop_path, num_heads, add_bias_kv, pre_transformer_ln):\n",
    "            simple_transformer = SimpleTransformer(\n",
    "                embed_dim = embed_dim,\n",
    "                num_blocks = num_blocks,\n",
    "                ffn_dropout_rate=0.0,\n",
    "                drop_path_rate = drop_path,\n",
    "                attn_target = \n",
    "                    MultiheadAttention(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    bias=True,\n",
    "                    add_bias_kv=add_bias_kv,\n",
    "                ),\n",
    "                # I already maked sure that the shape is aligned when using MultiheadAttention so no need of rearrangement now\n",
    "                pre_transformer_layer = nn.LayerNorm(embed_dim) if pre_transformer_ln else nn.Identity(),\n",
    "                post_transformer_layer = nn.Identity()\n",
    "            )\n",
    "            return simple_transformer\n",
    "        \n",
    "        modality_trunks = {}\n",
    "        modality_trunks[ModalityType.VISION] = instantiate_trunk(embed_dim = vision_embed_dim, num_blocks = vision_num_blocks, num_heads = vision_num_heads, drop_path = 0.0, pre_transformer_ln=True, add_bias_kv=False,)\n",
    "        modality_trunks[ModalityType.TEXT] = instantiate_trunk(embed_dim = text_embed_dim, num_blocks = text_num_blocks, num_heads = text_num_heads, drop_path = 0.0, pre_transformer_ln=False, add_bias_kv=False,)\n",
    "        \n",
    "        return nn.ModuleDict(modality_trunks)\n",
    "    \n",
    "    def _create_modality_heads(\n",
    "        self,\n",
    "        out_embed_dim,\n",
    "        vision_embed_dim,\n",
    "        text_embed_dim,\n",
    "    ):\n",
    "        modality_heads = {}\n",
    "\n",
    "        modality_heads[ModalityType.VISION] = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape = vision_embed_dim, eps=1e-6),\n",
    "            SelectElement(index = 0),\n",
    "            nn.Linear(vision_embed_dim, out_embed_dim, bias = False)\n",
    "        )\n",
    "\n",
    "        modality_heads[ModalityType.TEXT] = SelectEOSandProject(\n",
    "            proj = nn.Sequential(\n",
    "                nn.LayerNorm(normalized_shape=text_embed_dim, eps=1e-6),\n",
    "                nn.Linear(text_embed_dim, out_embed_dim, bias=False),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return nn.ModuleDict(modality_heads)\n",
    "\n",
    "    def _create_modality_postprocessors(self):\n",
    "        modality_postprocessors = {}\n",
    "\n",
    "        modality_postprocessors[ModalityType.VISION] = Normalize(dim=-1)\n",
    "        modality_postprocessors[ModalityType.TEXT] = nn.Sequential(\n",
    "            Normalize(dim=-1), LearnableLogitScaling(learnable=True)\n",
    "        )\n",
    "\n",
    "        return nn.ModuleDict(modality_postprocessors)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        outputs = {}\n",
    "        for modality_key, modality_value in inputs.items():\n",
    "            reduce_list = (modality_value.ndim >= 5)     # Because video's ndim is 5 (B, T, C, H, W)\n",
    "            if reduce_list:\n",
    "                B, S = modality_value.shape[:2]\n",
    "                modality_value = modality_value.reshape(B*S, *modality_value.shape[2:])\n",
    "            \n",
    "            if modality_value is not None:\n",
    "                modality_value = self.modality_preprocessors[modality_key](**{modality_key: modality_value}) # Access the forward function of the claasses\n",
    "\n",
    "                trunk_inputs = modality_value[\"trunk\"]\n",
    "                head_inputs = modality_value[\"head\"]\n",
    "                modality_value = self.modality_trunks[modality_key](**trunk_inputs)                # Access the forward function of the claasses\n",
    "                modality_value = self.modality_heads[modality_key](modality_value, **head_inputs) # Access the forward function of the claasses\n",
    "\n",
    "            if reduce_list:\n",
    "                modality_value = modality_value.reshape(B, S, -1)\n",
    "                modality_value = modality_value.mean(dim=1)\n",
    "\n",
    "            outputs[modality_key] = modality_value\n",
    "            \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb5a932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): RGBTProcessor(\n",
       "    (rgbt_stem): PatchEmbedGeneric(\n",
       "      (proj): Sequential(\n",
       "        (0): PadIm2Video()\n",
       "        (1): Conv3d(3, 1024, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (pos_embed_helper): SpatioTemporal_posEmbeddingHelper()\n",
       "  )\n",
       "  (text): TextPreprocessor(\n",
       "    (mask): tensor((77, 77), requires_grad=False)\n",
       "    \n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel = ImageBindModel()\n",
    "imagebindmodel.modality_preprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7927a618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[82, 17, 70, 60, 14, 99, 52, 13,  7, 91, 43, 80, 68, 16, 70, 22, 13, 14,\n",
       "         10, 48, 95, 14, 94, 35, 47, 61, 55, 43, 14, 74,  9, 29, 91, 11, 50, 20,\n",
       "         91, 90, 86, 30, 17, 86,  5, 93, 88, 12, 26, 73, 14, 59,  1, 49, 28, 90,\n",
       "         70, 79, 38, 81, 84, 50, 28, 68, 42, 94, 75,  2, 70, 10, 18, 80, 35,  5,\n",
       "         17, 43, 86, 26, 96],\n",
       "        [16,  5, 90, 40, 60, 46, 21, 51, 88, 52, 68, 32, 58, 95, 28, 71,  2, 40,\n",
       "         87, 25, 80,  3, 67, 30, 58, 64, 84, 55, 26, 15, 56,  9, 38, 57, 87, 41,\n",
       "         30, 72, 86, 84, 17, 17, 51, 71, 18, 86, 58,  1, 95, 61, 41,  2,  3, 58,\n",
       "         43, 66, 26, 74, 57, 19,  0, 99, 92,  7, 22, 42,  8, 65, 62, 81, 21, 74,\n",
       "         67, 13, 19, 80, 97]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 100\n",
    "context_length = 77\n",
    "embed_dim = 768\n",
    "\n",
    "# Sample input: batch of 1, padded or truncated to 77 tokens\n",
    "text = torch.randint(0, vocab_size, (2, context_length))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf9012e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trunk': {'tokens': tensor([[[ 0.0079, -0.0165, -0.0300,  ..., -0.0457, -0.0228,  0.0185],\n",
       "           [ 0.0387, -0.0072, -0.0253,  ...,  0.0064,  0.0061,  0.0076],\n",
       "           [-0.0428, -0.0018, -0.0102,  ..., -0.0243, -0.0410, -0.0098],\n",
       "           ...,\n",
       "           [-0.0368, -0.0145,  0.0055,  ..., -0.0097, -0.0285,  0.0355],\n",
       "           [-0.0196,  0.0123,  0.0011,  ..., -0.0122,  0.0154,  0.0118],\n",
       "           [ 0.0130, -0.0391, -0.0155,  ...,  0.0330, -0.0519, -0.0448]],\n",
       "  \n",
       "          [[ 0.0168, -0.0265, -0.0327,  ..., -0.0205,  0.0125,  0.0087],\n",
       "           [ 0.0185,  0.0153,  0.0095,  ...,  0.0276, -0.0311, -0.0119],\n",
       "           [ 0.0086,  0.0456,  0.0010,  ...,  0.0005, -0.0140, -0.0327],\n",
       "           ...,\n",
       "           [-0.0003, -0.0019, -0.0004,  ...,  0.0419, -0.0122,  0.0537],\n",
       "           [-0.0098, -0.0520,  0.0379,  ...,  0.0140,  0.0317,  0.0182],\n",
       "           [ 0.0003, -0.0311, -0.0242,  ...,  0.0317,  0.0162,  0.0165]]],\n",
       "         grad_fn=<AddBackward0>),\n",
       "  'attn_mask': tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "          [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "          [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "          [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]])},\n",
       " 'head': {'seq_len': tensor([ 5, 61])}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_preprocessors['text'](**{'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db46fad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): SimpleTransformer(\n",
       "    (pre_transformer_layer): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (post_transformer_layer): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (1): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (2): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (3): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (4): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (5): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (6): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (7): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (8): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (9): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (10): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (11): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (12): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (13): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (14): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (15): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (16): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (17): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (18): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (19): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (20): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (21): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (22): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (23): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text): SimpleTransformer(\n",
       "    (pre_transformer_layer): Identity()\n",
       "    (post_transformer_layer): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (1): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (2): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (3): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (4): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (5): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (6): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (7): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (8): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (9): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (10): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "      (11): BlockWithMasking(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layer_scale_gamma1): Identity()\n",
       "        (layer_scale_gamma2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_trunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3daa7cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (1): SelectElement()\n",
       "    (2): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  )\n",
       "  (text): SelectEOSandProject(\n",
       "    (proj): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3468611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (vision): Normalize()\n",
       "  (text): Sequential(\n",
       "    (0): Normalize()\n",
       "    (1): LearnableLogitScaling(logit_scale_init=14.285714285714285,learnable=True, max_logit_scale=100)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_postprocessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a09959cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, (1, 16, 16), 256)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_embed_dim = 1024\n",
    "kernel_size = (2, 14, 14)\n",
    "                     \n",
    "rgbt_stem = PatchEmbedGeneric(\n",
    "            proj_stem =\n",
    "            [\n",
    "                PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            \n",
    "            ]\n",
    "        )\n",
    "\n",
    "rgbt_stem.get_patch_layout([3, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c49df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "context_length = 4\n",
    "embed_dim = 8\n",
    "\n",
    "\n",
    "text = torch.randint(0, vocab_size, (2, context_length)) # text: [2, 77]\n",
    "image = torch.randn(2, 1024, 256, 224, 224)\n",
    "\n",
    "inputs = {\n",
    "    ModalityType.VISION: image,\n",
    "    ModalityType.TEXT: text\n",
    "}\n",
    "\n",
    "imagebindmodel = ImageBindModel()\n",
    "outputs = imagebindmodel(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63849c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleTransformer(\n",
       "  (pre_transformer_layer): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_transformer_layer): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (1): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (2): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (3): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (4): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (5): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (6): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (7): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (8): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (9): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (10): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (11): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (12): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (13): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (14): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (15): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (16): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (17): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (18): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (19): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (20): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (21): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (22): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "    (23): BlockWithMasking(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (layer_scale_gamma1): Identity()\n",
       "      (layer_scale_gamma2): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagebindmodel.modality_trunks['vision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['text'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
