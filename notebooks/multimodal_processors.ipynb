{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba29a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import math\n",
    "from timm.models.layers import trunc_normal_\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4ce0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLayer(\n",
      "  (running_mean): tensor((5,), requires_grad=False)\n",
      "  \n",
      "  (linear): Linear(in_features=1, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VerboseNNModule(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_readable_tensor_representation(name: str, tensor: torch.Tensor):\n",
    "        st = (\n",
    "            \"(\" + name + \"): \" + \"tensor(\" + str(tuple(tensor[1].shape)) + \", requires_grad=\" + str(tensor[1].requires_grad) + \")\\n\"\n",
    "            )\n",
    "        return st\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        named_modules = set()\n",
    "        for p in self.named_modules():\n",
    "            named_modules.update(p[0])\n",
    "        named_modules = list(named_modules)\n",
    "\n",
    "        string_repr = \"\"\n",
    "        for p in self.named_parameters():\n",
    "            name = p[0].split(\".\")[0]\n",
    "            if name in named_modules:\n",
    "                string_repr += self.get_readable_tensor_representation(name, p)\n",
    "        \n",
    "        for p in self.named_buffers():\n",
    "            name = p[0].split(\".\")[0]\n",
    "            string_repr += self.get_readable_tensor_representation(name, p)\n",
    "        \n",
    "        return string_repr\n",
    "\n",
    "class MyLayer(VerboseNNModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 2)\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(5))\n",
    "\n",
    "# Instantiate and print the model\n",
    "model = MyLayer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8aef52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_causal_attention_mask(context_length):\n",
    "    mask = torch.empty(context_length, context_length, requires_grad=False)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)\n",
    "    return mask\n",
    "\n",
    "build_causal_attention_mask(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94d826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor(VerboseNNModule):\n",
    "    def __init__(self, vocab_size: int, context_length: int, embed_dim: int, causual_mask: bool, \n",
    "                 supply_seq_len_to_head: bool = True, init_param_style: str = \"openclip\"):\n",
    "        \"\"\"\n",
    "        `vocab_size`: Number of tokens in your vocabulary.                 the number of words in your text, so we can map nn.Embedding\n",
    "\t    `context_length`: Maximum number of tokens per input sequence.     usually: 77\n",
    "\t    `embed_dim`: Dimensionality of each token embedding.               usually: 768\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.causual_mask = causual_mask\n",
    "        self.embed_dim = embed_dim\n",
    "        self.supply_seq_len_to_head = supply_seq_len_to_head\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.empty(1, context_length, embed_dim)\n",
    "        )\n",
    "        if causual_mask:\n",
    "            mask = build_causal_attention_mask(context_length)\n",
    "            self.register_buffer(\"mask\", mask) # register the mask as a buffer so it can be moved to the right device\n",
    "        \n",
    "        self.init_parameters(init_param_style)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, init_param_style = \"openclip\"):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        # I did'nt use init_param_style as I was too lazy to implment [CLS]\n",
    "    \n",
    "    def forward(self, text):\n",
    "        token_text = self.token_embedding(text)\n",
    "        token_text = token_text + self.pos_embed\n",
    "        \n",
    "        return_dict = {\n",
    "            \"trunk\": {\n",
    "                \"tokens\": token_text\n",
    "            },\n",
    "            \"head\": {},\n",
    "        }\n",
    "\n",
    "        if self.supply_seq_len_to_head:\n",
    "            text_lengths = text.argmax(dim = -1)\n",
    "            #  hacky and non-standard way of getting the sequence length.\n",
    "            return_dict[\"head\"] = {\n",
    "                \"seq_len\": text_lengths,\n",
    "            }\n",
    "        if self.causual_mask:\n",
    "            return_dict[\"trunk\"].update({\"attn_mask\": self.mask})\n",
    "        \n",
    "        return return_dict\n",
    "    \n",
    "vocab_size = 100\n",
    "context_length = 77\n",
    "embed_dim = 768\n",
    "\n",
    "# Sample input: batch of 1, padded or truncated to 77 tokens\n",
    "text = torch.randint(0, vocab_size, (2, context_length))  # shape [2, 77]\n",
    "\n",
    "text_processor = TextPreprocessor(\n",
    "    vocab_size=vocab_size,\n",
    "    context_length=context_length,\n",
    "    embed_dim=embed_dim,\n",
    "    causual_mask=True,\n",
    "    supply_seq_len_to_head=True\n",
    ")\n",
    "\n",
    "out = text_processor(text)\n",
    "print(out[\"trunk\"][\"tokens\"].shape)  # âžœ [2, 77, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1d3395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([77, 77])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"trunk\"][\"attn_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935c1cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42, 39])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"head\"][\"seq_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5912916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextPreprocessor(\n",
      "  (mask): tensor((77, 77), requires_grad=False)\n",
      "  \n",
      "  (token_embedding): Embedding(100, 768)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(text_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b41f67ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 15, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Im2Video(VerboseNNModule):\n",
    "    \"\"\" Converts image to video (Just adding T dimension lol)\"\"\"\n",
    "    def __init__(self, time_dim = 2):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if x.ndim == 5:\n",
    "            # Already includes T dimension\n",
    "            return x\n",
    "        if x.ndim == 4:\n",
    "            # Convert (B, C, H, W) -> (B, C, T, H, W)\n",
    "            return x.unsqueeze(dim = self.time_dim)\n",
    "        raise ValueError(f\"Dimension incorrect {x.shape}\")\n",
    "\n",
    "class PadIm2Video(Im2Video):\n",
    "    def __init__(self, ntimes, pad_type, time_dim=2):\n",
    "        super().__init__(time_dim=time_dim)\n",
    "        assert ntimes > 0\n",
    "        assert pad_type in [\"zero\", \"repeat\"]\n",
    "        self.ntimes = ntimes\n",
    "        self.pad_type = pad_type\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = super().forward(x) # (B, C, H, W) -> (B, C, T, H, W)\n",
    "        if x.shape[self.time_dim] == 1:\n",
    "            if self.pad_type == \"repeat\":\n",
    "                new_shape = [1] * len(x.shape)\n",
    "                new_shape[self.time_dim] = self.ntimes\n",
    "                x = x.repeat(new_shape)\n",
    "                return x\n",
    "            elif self.pad_type == \"zero\":\n",
    "                raise NotImplemented(f\"Todo: Need to implement this in the future\")\n",
    "        else: return x\n",
    "    \n",
    "x = torch.rand(1, 3, 224, 224)     \n",
    "padim2video = PadIm2Video(15, \"repeat\")\n",
    "padim2video(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9716d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PatchEmbedGeneric] After layer 0 (PadIm2Video): torch.Size([2, 3, 2, 224, 224])\n",
      "[PatchEmbedGeneric] After layer 1 (Conv3d): torch.Size([2, 256, 1, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 256])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PatchEmbedGeneric(nn.Module):\n",
    "    def __init__(self, proj_stem, norm_layer: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        if len(proj_stem) > 1:\n",
    "            self.proj = nn.Sequential(*proj_stem)\n",
    "        else:\n",
    "            # Special case to be able to load pre-trained models that were\n",
    "            # trained with a standard stem\n",
    "            self.proj = proj_stem[0]\n",
    "        self.norm_layer = norm_layer\n",
    "    \n",
    "    def get_patch_layout(self, image_size):\n",
    "        with torch.no_grad():\n",
    "            dummy_img = torch.zeros([1,] + image_size)      # 1, C, (T), H, W\n",
    "            dummy_out = self.proj(dummy_img)\n",
    "        # print(dummy_out.shape)\n",
    "        embed_dim = dummy_out.shape[1]                    # `embed_dim`    = C        \n",
    "        patch_layout = tuple(dummy_out.shape[2:])         # `patch_layout` = (T), H, W       \n",
    "        num_patches = np.prod(patch_layout)               # `num_patches`  = (T) * H * W       \n",
    "        return embed_dim, patch_layout, num_patches\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x = self.proj(x)                                  # B, C, H, W      -> B, C, (T), H, W \n",
    "        for i, layer in enumerate(self.proj):\n",
    "            x = layer(x)\n",
    "            print(f\"[PatchEmbedGeneric] After layer {i} ({layer.__class__.__name__}): {x.shape}\")\n",
    "            \n",
    "        x = x.flatten(2)                                  # B, C, (T), H, W -> B, C, (T)*H*W\n",
    "        x = x.transpose(1, 2)                             # B, C, (T)*H*W   -> B, (T)*H*W, C\n",
    "        if self.norm_layer is not None:\n",
    "            x = self.norm_layer(x)\n",
    "        return x\n",
    "\n",
    "## Testing\n",
    "\n",
    "kernel_size = (2, 14, 14)\n",
    "vision_embed_dim = 256\n",
    "proj_stem = [\n",
    "                PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "                nn.Conv3d(\n",
    "                    in_channels=3,\n",
    "                    kernel_size=kernel_size,\n",
    "                    out_channels=vision_embed_dim,\n",
    "                    stride=kernel_size,\n",
    "                    bias=False,\n",
    "                )\n",
    "            \n",
    "            ]\n",
    "\n",
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "out = patch_embed(x)\n",
    "out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f942073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "x = torch.randn(2, 3, 2, 224, 224)\n",
    "\n",
    "out = patch_embed(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0813c2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 256\n",
      "Patch layout: (1, 16, 16)\n",
      "Number of patches: 256\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 224, 224)\n",
    "embed_dim, patch_layout, num_patches = patch_embed.get_patch_layout(list(x.shape))\n",
    "\n",
    "print(f\"Embed dim: {embed_dim}\")\n",
    "print(f\"Patch layout: {patch_layout}\")\n",
    "print(f\"Number of patches: {num_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb127a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  8.2843e-01,  ...,  1.0000e+00,\n",
      "           1.0243e-04,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  9.2799e-01,  ...,  1.0000e+00,\n",
      "           2.0486e-04,  1.0000e+00],\n",
      "         ...,\n",
      "         [-9.7846e-01, -2.0645e-01, -6.9584e-02,  ...,  9.9980e-01,\n",
      "           1.9767e-02,  9.9980e-01],\n",
      "         [-7.0239e-01,  7.1180e-01,  7.8745e-01,  ...,  9.9979e-01,\n",
      "           1.9870e-02,  9.9980e-01],\n",
      "         [ 2.1945e-01,  9.7562e-01,  9.5167e-01,  ...,  9.9979e-01,\n",
      "           1.9972e-02,  9.9980e-01]]])\n"
     ]
    }
   ],
   "source": [
    "def get_sinusoid_encoding_table(n_position, d_hid):\n",
    "    \"\"\"Sinusoid position encoding table\"\"\"\n",
    "\n",
    "    # TODO: make it with torch instead of numpy\n",
    "    def get_position_angle_vec(position):\n",
    "        return [\n",
    "            position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n",
    "            for hid_j in range(d_hid)\n",
    "        ]\n",
    "\n",
    "    sinusoid_table = np.array(\n",
    "        [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n",
    "    )\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
    "\n",
    "pos_encoding = get_sinusoid_encoding_table(n_position=196, d_hid=768)\n",
    "print(pos_encoding.shape)  # Output: (1, 4, 8)\n",
    "print(pos_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1390ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def interpolate_pos_encoding(npatch_per_image, pos_embed, first_patch_idx: int = 1): \n",
    "    \"\"\"\n",
    "    Interpolates the patch positional embeddings to match the new number of patches.\n",
    "    Args:\n",
    "        npatch_per_image (int): New number of patches (excluding CLS if present).\n",
    "        pos_embed (Tensor): Shape (1, N+1, D) or (1, N, D).\n",
    "        first_patch_idx (int): 1 if CLS token is present, else 0.\n",
    "    Returns:\n",
    "        Interpolated pos_embed of shape (1, npatch_per_image + first_patch_idx, D)\n",
    "    \"\"\"\n",
    "    assert first_patch_idx in {0, 1}, \"CLS token can be present (1) or absent (0)\"\n",
    "    D = pos_embed.shape[-1]\n",
    "    class_emb = pos_embed[:, :first_patch_idx]\n",
    "    patch_pos_embed = pos_embed[:, first_patch_idx:]\n",
    "\n",
    "    old_npatch = patch_pos_embed.shape[1]\n",
    "\n",
    "    # Interpolate positional embeddings\n",
    "    patch_pos_embed = patch_pos_embed.permute(0, 2, 1)  # (1, D, N)\n",
    "    patch_pos_embed = F.interpolate(patch_pos_embed, size=npatch_per_image, mode='linear', align_corners=False)\n",
    "    patch_pos_embed = patch_pos_embed.permute(0, 2, 1)  # (1, N_new, D)\n",
    "\n",
    "    return torch.cat((class_emb, patch_pos_embed), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8659b286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_pos_embedding(npatch_per_image, pos_embed, first_patch_idx: int = 1):\n",
    "    return interpolate_pos_encoding(npatch_per_image, pos_embed, first_patch_idx)\n",
    "\n",
    "_get_pos_embedding(npatch_per_image = 196, pos_embed = torch.rand(1, 197, 768), first_patch_idx=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52301db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporal_posEmbeddingHelper(VerboseNNModule):\n",
    "    def __init__(self, num_patches: int, num_cls_tokens: int, embed_dim: int, learnable: bool = True):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.num_cls_tokens = num_cls_tokens\n",
    "        self.embed_dim = embed_dim\n",
    "        self.learnable = learnable\n",
    "\n",
    "        self.num_tokens = num_patches + num_cls_tokens\n",
    "\n",
    "        if learnable:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                                torch.zeros(1, self.num_tokens, embed_dim)\n",
    "                            )\n",
    "            trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "        else: self.register_buffer(\n",
    "            \"pos_embed\", get_sinusoid_encoding_table(n_position = self.num_tokens, d_hid = embed_dim)\n",
    "            )\n",
    "    \n",
    "    def get_pos_embedding(self, all_vision_tokens):\n",
    "        pos_embed = _get_pos_embedding(\n",
    "            npatch_per_image = all_vision_tokens.size(1) - self.num_cls_tokens,\n",
    "            pos_embed=self.pos_embed,\n",
    "            first_patch_idx=self.num_cls_tokens,\n",
    "        )\n",
    "        return pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5a48710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17, 768])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches = 16\n",
    "num_cls_tokens = 1\n",
    "embed_dim = 768\n",
    "\n",
    "# Fake patch embeddings for 2 images\n",
    "batch_size = 2\n",
    "patch_embeddings = torch.randn(batch_size, num_patches, embed_dim)  # [2, 16, 768]\n",
    "\n",
    "# CLS token (typically learned)\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))  # [1, 1, 768]\n",
    "cls_tokens = cls_token.expand(batch_size, -1, -1)       # [2, 1, 768]\n",
    "\n",
    "# Combine CLS and patches\n",
    "all_vision_tokens = torch.cat([cls_tokens, patch_embeddings], dim=1)  # [2, 17, 768]\n",
    "all_vision_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2612608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 768])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_helper = SpatioTemporal_posEmbeddingHelper(\n",
    "    num_patches=num_patches,\n",
    "    num_cls_tokens=num_cls_tokens,\n",
    "    embed_dim=embed_dim,\n",
    "    learnable=False  # or True if you want learnable positions\n",
    ")\n",
    "\n",
    "vision_input = torch.randn(batch_size, 3, 224, 224)  # dummy input\n",
    "pos_embed = pos_helper.get_pos_embedding(all_vision_tokens = all_vision_tokens)  # [1, 17, 768] (broadcastable)\n",
    "pos_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07090a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGBTProcessor(VerboseNNModule):\n",
    "    def __init__(self, rgbt_stem: PatchEmbedGeneric, img_size: Tuple = [3, 224, 224],\n",
    "                 num_cls_token: int = 1, pos_embed_fn: SpatioTemporal_posEmbeddingHelper = None, \n",
    "                 use_type_embed: bool = False, init_param_style: str = \"openclip\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim, self.patches_layout, self.num_patches = rgbt_stem.get_patch_layout(img_size)\n",
    "        self.num_cls_token = num_cls_token\n",
    "        self.use_type_embed = use_type_embed\n",
    "        self.init_param_style = init_param_style\n",
    "        self.use_pos_embed = pos_embed_fn is not None\n",
    "        self.rgbt_stem = rgbt_stem\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            self.pos_embed_helper = pos_embed_fn(\n",
    "                num_patches = self.num_patches,\n",
    "                num_cls_tokens = self.num_cls_token,\n",
    "                embed_dim = self.embed_dim,\n",
    "                learnable = True\n",
    "            )\n",
    "        \n",
    "        if num_cls_token > 0:\n",
    "            self.cls_tokens = nn.Parameter(\n",
    "                torch.zeros(1, self.num_cls_token, self.embed_dim)\n",
    "            )\n",
    "        if self.use_type_embed: # The model learns to adjust type_embed so that it provides differentiation for different modalities\n",
    "            self.type_embed = nn.Parameter(\n",
    "                torch.zeros(1, 1, self.embed_dim)\n",
    "            )\n",
    "        \n",
    "        self.init_parameters(init_param_style)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def init_parameters(self, parameter_style):\n",
    "        if parameter_style == \"openclip\":\n",
    "            # OpenCLIP style initialization\n",
    "            scale = self.embed_dim ** -0.5\n",
    "        \n",
    "            if self.use_type_embed:\n",
    "                nn.init.normal_(self.pos_embed_helper.pos_embed)\n",
    "                self.pos_embed_helper.pos_embed *= scale\n",
    "            \n",
    "            if self.num_cls_token > 0:\n",
    "                nn.init.normal_(self.cls_tokens)\n",
    "                self.cls_tokens *= scale\n",
    "        \n",
    "        elif parameter_style == \"vit\":\n",
    "            self.cls_tokens.data.fill_(0)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown init {parameter_style}\")\n",
    "        \n",
    "        if self.use_type_embed:\n",
    "            nn.init.normal_(self.type_embed)\n",
    "        \n",
    "    def tokenize_input_and_cls_pos(self, input, stem):\n",
    "        print(\"input to patchem: \", input.shape)\n",
    "        print(stem)\n",
    "        tokens = stem(input)\n",
    "        print(\"Tokens before: \", tokens.shape)\n",
    "        assert tokens.ndim == 3\n",
    "        assert tokens.shape[-1] == self.embed_dim\n",
    "\n",
    "        B = tokens.shape[0] # batch size\n",
    "        \n",
    "        if self.num_cls_token > 0:\n",
    "            cls_tokens = self.cls_tokens.expand(B, -1, -1)   # Making sure Batches are matching or shape mismatch might occur\n",
    "            tokens = torch.cat([cls_tokens, tokens], dim=1)\n",
    "\n",
    "        if self.use_pos_embed:\n",
    "            pos_embed = self.pos_embed_helper.get_pos_embedding(all_vision_tokens = tokens)\n",
    "            print(\"Tokens after: \", tokens.shape)\n",
    "            print(\"pos_embed: \", pos_embed.shape)\n",
    "\n",
    "            tokens = tokens + pos_embed\n",
    "        \n",
    "        if self.use_type_embed:\n",
    "            tokens = tokens + self.type_embed.expand(B, -1, -1)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def forward(self, vision = None):\n",
    "        vision_tokens = self.tokenize_input_and_cls_pos(input = vision, stem = self.rgbt_stem)\n",
    "        return_dict = {\n",
    "                        \"trunk\": {\n",
    "                            \"tokens\": vision_tokens\n",
    "                        },\n",
    "                        \"head\": {}\n",
    "                    }\n",
    "        return return_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df3b73e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to patchem:  torch.Size([2, 3, 224, 224])\n",
      "PatchEmbedGeneric(\n",
      "  (proj): Sequential(\n",
      "    (0): PadIm2Video()\n",
      "    (1): Conv3d(3, 256, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "  )\n",
      ")\n",
      "[PatchEmbedGeneric] After layer 0 (PadIm2Video): torch.Size([2, 3, 2, 224, 224])\n",
      "[PatchEmbedGeneric] After layer 1 (Conv3d): torch.Size([2, 256, 1, 16, 16])\n",
      "Tokens before:  torch.Size([2, 256, 256])\n",
      "Tokens after:  torch.Size([2, 257, 256])\n",
      "pos_embed:  torch.Size([1, 257, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 257, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = (2, 14, 14)\n",
    "vision_embed_dim = 256\n",
    "\n",
    "proj_stem =[ \n",
    "    PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "    nn.Conv3d(\n",
    "        in_channels=3,\n",
    "        kernel_size=kernel_size,\n",
    "        out_channels=vision_embed_dim,\n",
    "        stride=kernel_size,\n",
    "        bias=False,\n",
    "    )\n",
    "    ]\n",
    "\n",
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "\n",
    "rgbt_processor = RGBTProcessor(rgbt_stem = patch_embed,\n",
    "              pos_embed_fn = SpatioTemporal_posEmbeddingHelper, use_type_embed = True,\n",
    "              img_size = [3, 2, 224, 224]\n",
    "              )\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "out = rgbt_processor(x)\n",
    "out[\"trunk\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28f53cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_stem =[ PadIm2Video(ntimes=2, pad_type=\"repeat\"), \n",
    "    nn.Conv3d(\n",
    "        in_channels=3,\n",
    "        kernel_size=kernel_size,\n",
    "        out_channels=vision_embed_dim,\n",
    "        stride=kernel_size,\n",
    "        bias=False,\n",
    "    )\n",
    "    ]\n",
    "\n",
    "patch_embed = PatchEmbedGeneric(proj_stem)\n",
    "\n",
    "# pos_helper = SpatioTemporal_posEmbeddingHelper(\n",
    "#     num_patches=num_patches,\n",
    "#     num_cls_tokens=num_cls_tokens,\n",
    "#     embed_dim=embed_dim,\n",
    "#     learnable=False  # or True if you want learnable positions\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40734a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to patchem:  torch.Size([2, 3, 224, 224])\n",
      "Tokens after:  torch.Size([2, 257, 256])\n",
      "pos_embed:  torch.Size([1, 257, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 257, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf04b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbt_processor.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c3257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 50177, 3])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"trunk\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e523136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0903dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/xlr6tgyx4t980qpxnnrs12kc0000gn/T/ipykernel_22456/676307527.py:1: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  nn.init.normal(torch.rand(1,2,3))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0355,  0.7191, -0.7407],\n",
       "         [-1.3586, -0.0918, -1.3073]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal(torch.rand(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RGBTProcessor(\n",
       "  (rgbt_stem): PatchEmbedGeneric(\n",
       "    (proj): Sequential(\n",
       "      (0): Linear(in_features=224, out_features=224, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGBTProcessor(patch_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fcc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2100e9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
